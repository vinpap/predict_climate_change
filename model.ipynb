{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98c453df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f875ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# On charge le fichier csv qui se trouve dans un sous-dossier 'data'\n",
    "data = pd.read_csv(\"./data/GlobalTemperatures.csv\")\n",
    "# On charge les températures moyennes (terre et océan confondus) dans une série de données Pandas,\n",
    "# à partir de la 1300ème ligne (il n'y a aucune donnée avant cela).\n",
    "# On réalise aussi une interpolation pour remplir automatiquement certaines valeurs manquantes\n",
    "temperature = data[\"LandAndOceanAverageTemperature\"].interpolate(limit_direction=\"backward\")[1300:]\n",
    "dates = pd.to_datetime(data[\"dt\"])[1300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def make_stationary(time_series):\n",
    "    \"\"\"\n",
    "    Makes a time series stationary while the p-value computed using the ADF test is higher than 0.05\n",
    "    \"\"\"\n",
    "    test_results = adfuller(time_series)\n",
    "    p_value = test_results[1]\n",
    "    if p_value > 0.05: # i.e. if the data is not stationary\n",
    "        # Differentiating until the p-value goes under 0.05\n",
    "        diff_data = time_series.copy()\n",
    "        for degree in range(1, 10):\n",
    "            diff_data = diff_data.diff().dropna()\n",
    "            if adfuller(diff_data)[1] <= 0.05:\n",
    "                return diff_data, degree\n",
    "        \n",
    "        raise ValueError(f\"Unable to stationarize data after diferentiating {degree} times\")\n",
    "    return time_series\n",
    "\n",
    "_, diff_degree = make_stationary(temperature)\n",
    "print(f\"Degrees of differentiating: {diff_degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b252bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.model_selection import train_test_split\n",
    "\n",
    "# On commence par séparer nos jeux d'entraînement et de test\n",
    "training_samples_count = int(len(dates) * 0.9)\n",
    "train, test = train_test_split(temperature, train_size=training_samples_count)\n",
    "train_dates, future_dates = train_test_split(dates, train_size=training_samples_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161cbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recherche des meilleurs paramètres pour notre ARIMA\n",
    "# Comme nous avons déjà déterminé que notre degré de différenciation\n",
    "# est de 1, nous pouvons le préciser à auto_arima pour gagner du temps\n",
    "# d'exécution\n",
    "from pmdarima import auto_arima\n",
    "# On choisit une valeur de 12 pour la saisonnalité car nos données\n",
    "# sont des moyennes mensuelles\n",
    "best_model = auto_arima(train, d=1, seasonal=True, m=12, stepwise=False, n_jobs=-1, trace=True)\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dab863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.arima import ARIMA\n",
    "\n",
    "# On instancie notre modèle avec les hyperparamètres définis plus tôt\n",
    "model = ARIMA(\n",
    "    order=(1, 1, 1),\n",
    "    seasonal_order=(2, 0, 1, 12)\n",
    "               )\n",
    "model.fit(train)\n",
    "\n",
    "predictions_count = len(test)\n",
    "# On réalise des prédictions à comparer avec les valeurs de test\n",
    "forecast, confidence_intervals = model.predict(predictions_count, return_conf_int=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab0e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding confidence intervals\n",
    "ci_dict = {\n",
    "    \"low\": confidence_intervals[:, 0],\n",
    "    \"high\": confidence_intervals[:, 1]\n",
    "}\n",
    "confidence_intervals_df = pd.DataFrame(ci_dict)\n",
    "confidence_intervals_df.index = future_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1e899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(len(train_dates) + len(future_dates))\n",
    "trend = np.polyfit(x, pd.concat([train, forecast]), 1)\n",
    "\n",
    "\n",
    "trendpoly = np.poly1d(trend)\n",
    "plt.plot(dates, temperature, c='blue')\n",
    "plt.plot(future_dates, forecast, c='green')\n",
    "plt.plot(dates, trendpoly(x), c=\"red\")\n",
    "plt.fill_between(confidence_intervals_df.index, confidence_intervals_df['low'], confidence_intervals_df['high'], alpha=0.9, color='orange', label=\"Confidence intervals\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ef3460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "\n",
    "\n",
    "# Computing yearly average in order to increase readibility\n",
    "temperature.index = dates\n",
    "yearly_temperature = temperature.rolling(window=12, step=12, center=False).mean().dropna()\n",
    "\n",
    "train.index = train_dates\n",
    "yearly_train = train.rolling(window=12, step=12, center=False).mean().dropna()\n",
    "\n",
    "forecast.index = future_dates\n",
    "yearly_forecast = forecast.rolling(window=12, step=12, center=False).mean().dropna()\n",
    "\n",
    "train_and_forecast = pd.concat([yearly_train, yearly_forecast])\n",
    "x = np.arange(len(train_and_forecast))\n",
    "trend = np.polyfit(x, train_and_forecast, 1)\n",
    "trendpoly = np.poly1d(trend)\n",
    "\n",
    "yearly_train_dates = pd.Series(yearly_train.index)\n",
    "yearly_forecast_dates = pd.Series(yearly_forecast.index)\n",
    "all_dates = pd.concat([yearly_train_dates, yearly_forecast_dates])\n",
    "\n",
    "yearly_confidence_intervals_df = confidence_intervals_df.rolling(window=12, step=12, center=False).mean().dropna()\n",
    "\n",
    "plt.plot(yearly_temperature, c='blue', label='Températures annuelles réelles')\n",
    "plt.plot(yearly_forecast_dates, yearly_forecast.values, c='green', label='Températures annuelles prédites')\n",
    "#plt.plot(train_and_forecast.index, trendpoly(x), c=\"red\", label='Forecast trend')\n",
    "plt.fill_between(yearly_confidence_intervals_df.index, yearly_confidence_intervals_df['low'], yearly_confidence_intervals_df['high'], alpha=0.9, color='orange', label=\"Intervalles de confiance\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.gcf().autofmt_xdate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d944e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la RMSE\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse(true_values, predicted_values):\n",
    "    \"\"\"\n",
    "    Computes the Root Mean-Squared Error of the predicted values with regards to the actual values.\n",
    "    \"\"\"\n",
    "    return math.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "\n",
    "# Exemple avec nos prédictions :\n",
    "print(f\"RMSE : {rmse(test, forecast)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10ba223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ec5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99191a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bedbbca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f27b931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dad84ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0752787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b06ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e092775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b3406ee",
   "metadata": {},
   "source": [
    "## Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f435b565-6731-4d94-bd20-8c513b32ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pmdarima import auto_arima\n",
    "from pmdarima.arima import ARIMA\n",
    "from pmdarima.model_selection import train_test_split\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SPLIT_DATASET = True # Use this if you want to perform tests on parts of the time series\n",
    "DATASET_SPLIT_RATIO = 0.85 # If SPLIT_DATASET, is set to True, defines how much of the data should be used for training\n",
    "FORECAST_LENGTH = 120 # Number of forecast samples to generate if SPLIT_DATASET is set to False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stationary(time_series):\n",
    "    \"\"\"\n",
    "    Make a time series stationary if the p-value computed using the ADF test is higher than 0.05\n",
    "    \"\"\"\n",
    "    test_results = adfuller(time_series)\n",
    "    p_value = test_results[1]\n",
    "    if p_value > 0.05: # i.e. if the data is not stationary\n",
    "        # Differentiating until the p-value goes under 0.05\n",
    "        diff_data = time_series.copy()\n",
    "        for degree in range(1, 10):\n",
    "            diff_data = diff_data.diff().dropna()\n",
    "            if adfuller(diff_data)[1] <= 0.05:\n",
    "                return diff_data, degree\n",
    "        \n",
    "        raise ValueError(f\"Unable to stationarize data after diferentiating {degree} times\")\n",
    "    return time_series\n",
    "\n",
    "\n",
    "\n",
    "def decompose_data(time_series):\n",
    "    \"\"\"\n",
    "    Decompose our data following an additive model\n",
    "    \"\"\"\n",
    "    additive_decomposition = seasonal_decompose(time_series.values, model='additive', period=12)\n",
    "    return {\n",
    "        \"trend\": additive_decomposition.trend,\n",
    "        \"seasonal\": additive_decomposition.seasonal,\n",
    "        \"residual\": additive_decomposition.resid\n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16eee3f",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b73a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/GlobalTemperatures.csv\")\n",
    "print(data.head())\n",
    "# Getting rid of empty values (the first 1300 or so are missing)\n",
    "temp = data[\"LandAndOceanAverageTemperature\"].interpolate(limit_direction=\"backward\")[1300:]\n",
    "dates = pd.to_datetime(data[\"dt\"])[1300:]\n",
    "\n",
    "if SPLIT_DATASET:\n",
    "    TRAINING_SAMPLES_COUNT = int(len(dates) * DATASET_SPLIT_RATIO)\n",
    "    train, test = train_test_split(temp, train_size=TRAINING_SAMPLES_COUNT)\n",
    "    train_dates, future_dates = train_test_split(dates, train_size=TRAINING_SAMPLES_COUNT)\n",
    "\n",
    "\n",
    "else:\n",
    "    train = temp\n",
    "    train_dates = dates\n",
    "\n",
    "# Making our series stationary\n",
    "train_components = decompose_data(train)\n",
    "detrended_train = train.values - train_components[\"trend\"]\n",
    "trend_train = pd.Series(train_components[\"trend\"])\n",
    "detrended_train = pd.Series(detrended_train).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59677ec5",
   "metadata": {},
   "source": [
    "## Recherche des meilleurs paramètres et entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b5de30",
   "metadata": {},
   "source": [
    "Paramètres du meilleur modèle :\n",
    "Best model:  ARIMA(1,1,1)(2,0,2)[12] intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfa1b83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e99165",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_train = trend_train.dropna()\n",
    "print(np.array(trend_train.index).reshape(-1, 1))\n",
    "print(np.array(trend_train.values))\n",
    "\n",
    "#model = auto_arima(train, seasonal=True, m=12, trace=True)\n",
    "#print(str(model.summary()))\n",
    "model = ARIMA(order=(1, 1, 1), seasonal_order=(2, 0, 2, 12))\n",
    "model.fit(detrended_train)\n",
    "\n",
    "# ENTRAÎNER UN MODÈLE DE RÉGRESSION SUR LA TENDANCE ICI :\n",
    "trend_model = SGDRegressor()\n",
    "trend_model.fit(np.array(trend_train.index).reshape(-1, 1), trend_train.interpolate().values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32659a3f",
   "metadata": {},
   "source": [
    "## Prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6f4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPLIT_DATASET:\n",
    "    detrended_forecasts = model.predict(len(test))  # predict N steps into the future\n",
    "    future_index = np.arange(trend_train.index[-1]+1, trend_train.index[-1]+1 + len(test)).reshape(-1, 1)\n",
    "    trend_forecasts = trend_model.predict(future_index)\n",
    "\n",
    "    forecasts = detrended_forecasts + trend_forecasts\n",
    "    print(forecasts)\n",
    "else:\n",
    "    forecasts = model.predict(FORECAST_LENGTH)  # predict 10 years into the future\n",
    "\n",
    "forecasts = pd.Series(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b358ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "490574cf",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d1b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the forecasts (blue=train, green=forecasts)\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "\n",
    "if SPLIT_DATASET:\n",
    "    x = np.arange(len(train) + len(forecasts))\n",
    "    trend = np.polyfit(x, pd.concat([train, forecasts]), 2)\n",
    "\n",
    "else:\n",
    "    # Generating extra dates for the forecast\n",
    "    start_date = dates.iloc[-1] + pd.DateOffset(months=1)\n",
    "    future_dates = pd.date_range(start_date, periods=FORECAST_LENGTH, freq=\"M\").to_series()\n",
    "    all_dates = pd.concat([dates, future_dates])\n",
    "    x = np.arange(len(train) + len(forecasts))\n",
    "    trend = np.polyfit(x, pd.concat([train, forecasts]), 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trendpoly = np.poly1d(trend)\n",
    "plt.plot(train_dates, train, c='blue')\n",
    "plt.plot(future_dates, forecasts, c='green')\n",
    "plt.plot(all_dates, trendpoly(x), c=\"red\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd10f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reintegrating trends + comparing forecasts to actual values\n",
    "\n",
    "if SPLIT_DATASET:\n",
    "\n",
    "    trend_data = pd.Series(components[\"trend\"])\n",
    "    all_temp = pd.concat([train, forecasts]).reset_index().drop(columns=\"index\")[0]\n",
    "\n",
    "    full_data = all_temp + trend_data\n",
    "\n",
    "\n",
    "detrended_data = {\n",
    "    \"date\": all_dates,\n",
    "    \"detrended_temp\": pd.concat([train, forecasts])\n",
    "}\n",
    "\n",
    "detrended_df = pd.DataFrame(detrended_data)\n",
    "print(detrended_df.shape)\n",
    "plt.plot(detrended_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ba7276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b55943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation en faisant des moyennes de température annuelles, pour plus de visibilité\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "train.index = train_dates\n",
    "yearly_train = train.rolling(window=12, step=12, center=True).mean().dropna()\n",
    "\n",
    "forecasts.index = future_dates\n",
    "yearly_forecasts = forecasts.rolling(window=12, step=12, center=True).mean().dropna()\n",
    "\n",
    "all_data = pd.concat([yearly_train, yearly_forecasts])\n",
    "x = np.arange(len(all_data))\n",
    "\n",
    "plt.plot(all_data)\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
