\documentclass[french]{article}
\usepackage[T1]{fontenc}
\usepackage{fullpage}
\usepackage{babel}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[justification=centering]{caption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{parskip}

\lstset{
    inputencoding = utf8,  % Input encoding
    extendedchars = true,  % Extended ASCII
    literate      =        % Support additional characters
      {á}{{\'a}}1  {é}{{\'e}}1  {í}{{\'i}}1 {ó}{{\'o}}1  {ú}{{\'u}}1
      {Á}{{\'A}}1  {É}{{\'E}}1  {Í}{{\'I}}1 {Ó}{{\'O}}1  {Ú}{{\'U}}1
      {à}{{\`a}}1  {è}{{\`e}}1  {ì}{{\`i}}1 {ò}{{\`o}}1  {ù}{{\`u}}1
      {À}{{\`A}}1  {È}{{\`E}}1  {Ì}{{\`I}}1 {Ò}{{\`O}}1  {Ù}{{\`U}}1
      {ä}{{\"a}}1  {ë}{{\"e}}1  {ï}{{\"i}}1 {ö}{{\"o}}1  {ü}{{\"u}}1
      {Ä}{{\"A}}1  {Ë}{{\"E}}1  {Ï}{{\"I}}1 {Ö}{{\"O}}1  {Ü}{{\"U}}1
      {â}{{\^a}}1  {ê}{{\^e}}1  {î}{{\^i}}1 {ô}{{\^o}}1  {û}{{\^u}}1
      {Â}{{\^A}}1  {Ê}{{\^E}}1  {Î}{{\^I}}1 {Ô}{{\^O}}1  {Û}{{\^U}}1
      {œ}{{\oe}}1  {Œ}{{\OE}}1  {æ}{{\ae}}1 {Æ}{{\AE}}1  {ß}{{\ss}}1
      {ẞ}{{\SS}}1  {ç}{{\c{c}}}1 {Ç}{{\c{C}}}1 {ø}{{\o}}1  {Ø}{{\O}}1
      {å}{{\aa}}1  {Å}{{\AA}}1  {ã}{{\~a}}1  {õ}{{\~o}}1 {Ã}{{\~A}}1
      {Õ}{{\~O}}1  {ñ}{{\~n}}1  {Ñ}{{\~N}}1  {¿}{{?`}}1  {¡}{{!`}}1
      {°}{{\textdegree}}1 {º}{{\textordmasculine}}1 {ª}{{\textordfeminine}}1
      {£}{{\pounds}}1  {©}{{\copyright}}1  {®}{{\textregistered}}1
      {«}{{\guillemotleft}}1  {»}{{\guillemotright}}1  {Ð}{{\DH}}1  {ð}{{\dh}}1
      {Ý}{{\'Y}}1    {ý}{{\'y}}1    {Þ}{{\TH}}1    {þ}{{\th}}1    {Ă}{{\u{A}}}1
      {ă}{{\u{a}}}1  {Ą}{{\k{A}}}1  {ą}{{\k{a}}}1  {Ć}{{\'C}}1    {ć}{{\'c}}1
      {Č}{{\v{C}}}1  {č}{{\v{c}}}1  {Ď}{{\v{D}}}1  {ď}{{\v{d}}}1  {Đ}{{\DJ}}1
      {đ}{{\dj}}1    {Ė}{{\.{E}}}1  {ė}{{\.{e}}}1  {Ę}{{\k{E}}}1  {ę}{{\k{e}}}1
      {Ě}{{\v{E}}}1  {ě}{{\v{e}}}1  {Ğ}{{\u{G}}}1  {ğ}{{\u{g}}}1  {Ĩ}{{\~I}}1
      {ĩ}{{\~\i}}1   {Į}{{\k{I}}}1  {į}{{\k{i}}}1  {İ}{{\.{I}}}1  {ı}{{\i}}1
      {Ĺ}{{\'L}}1    {ĺ}{{\'l}}1    {Ľ}{{\v{L}}}1  {ľ}{{\v{l}}}1  {Ł}{{\L{}}}1
      {ł}{{\l{}}}1   {Ń}{{\'N}}1    {ń}{{\'n}}1    {Ň}{{\v{N}}}1  {ň}{{\v{n}}}1
      {Ő}{{\H{O}}}1  {ő}{{\H{o}}}1  {Ŕ}{{\'{R}}}1  {ŕ}{{\'{r}}}1  {Ř}{{\v{R}}}1
      {ř}{{\v{r}}}1  {Ś}{{\'S}}1    {ś}{{\'s}}1    {Ş}{{\c{S}}}1  {ş}{{\c{s}}}1
      {Š}{{\v{S}}}1  {š}{{\v{s}}}1  {Ť}{{\v{T}}}1  {ť}{{\v{t}}}1  {Ũ}{{\~U}}1
      {ũ}{{\~u}}1    {Ū}{{\={U}}}1  {ū}{{\={u}}}1  {Ů}{{\r{U}}}1  {ů}{{\r{u}}}1
      {Ű}{{\H{U}}}1  {ű}{{\H{u}}}1  {Ų}{{\k{U}}}1  {ų}{{\k{u}}}1  {Ź}{{\'Z}}1
      {ź}{{\'z}}1    {Ż}{{\.Z}}1    {ż}{{\.z}}1    {Ž}{{\v{Z}}}1
      % ¿ and ¡ are not correctly displayed if inconsolata font is used
      % together with the lstlisting environment. Consider typing code in
      % external files and using \lstinputlisting to display them instead.      
  }



\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}
\setlength{\parindent}{0pt}

\hypersetup{
  colorlinks=true,
  linkcolor=black,
  urlcolor=blue
}

\graphicspath{ {./img/} }
\title{%
    \huge Prédire les changements climatiques  \\
    \bigskip
    \large E2 - Cas pratique \\ 
    Développeur en Intelligence Artificielle,
    titre professionnel enregistré au RNCP - École IA Microsoft by Simplon
    \vfill
    \includegraphics[width=14cm]{global_warming.jpg}
    \vfill}
\date{12 février 2024}
\author{par Vincent Papelard}

\begin{document}
    \renewcommand{\contentsname}{Table des Matières}
    \renewcommand{\refname}{Références}
    \maketitle
    \pagenumbering{arabic}
    \pagenumbering{gobble}
    \newpage
    \tableofcontents
    \newpage
    \pagenumbering{arabic}

    \section*{Introduction}

    Un grand organisme de prédiction météorologique nous contacte. 
    Afin de prédire au mieux l'évolution du climat à échelle mondiale et dans le but de sensibiliser la population au réchaffement climatique, ses dirigeants souhaitent remplacer leurs modèles de prédiction vieillissants par un modèle d'IA moderne en cherchant la meilleure précision possible. Plus spécifiquement, ils nous demandent un modèle capable de prédire l'évolution d'une série de températures sur de longues périodes. 
    
    Le code associé à ce dossier est disponible \href{https://github.com/vinpap/predict_climate_change}{sur GitHub}.

    \addcontentsline{toc}{section}{Introduction}

    \section{Contraintes techniques et objectifs}

    L'objectif que nous donne notre client est de lui fournir un modèle de prédiction de températures basé sur une \textbf{série de températures mensuelles moyennes}. Bien que notre client souhaite utiliser ce modèle pour prédire des températures moyennes à l'échelle du monde entier, il n'exclut pas de s'en servir pour prédire des températures locales à l'avenir (par exemple à l'échelle d'une ville ou d'une région). Par conséquent, \textbf{notre client souhaite pouvoir réentraîner lui-même le modèle fourni pour l'utiliser avec d'autres données}.

    Par ailleurs, notre client nous impose les contraintes suivantes :
    \begin{itemize}
        \item le modèle doit être utilisable via une API. Les techniciens de notre client doivent pouvoir utiliser cette API pour réaliser des prédictions, évaluer les performances du modèle et le réentraîner
        \item notre client dispose de serveurs adaptés à la mise en place de modèles d'intelligence artificielle exigeants en ressource. Par conséquent, le choix du modèle d'intelligence artificielle à utiliser ne devrait pas exclure les modèles nécessitant beaucoup de puissance de calcul. En revanche, notre client insiste sur le fait que le modèle retenu doit être \textbf{celui qui offre les prédictions les plus précises} afin d'éviter les erreurs de prévision et pour que le public continue à faire confiance aux organismes météorologiques.
        \item des instructions expliquant comment installer et utiliser le modèle sur les serveurs de notre client devront être fournies.
    \end{itemize}

    \section{Jeu de données}
    
    Afin de faire la démonstration du modèle retenu, nous utiliserons \href{https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data}{ce dataset}. Il rencense les températures mensuelles moyennes sur Terre depuis le XVIIIè siècle, à échelle mondiale et en des points du globe spécifiques. Ce jeu de données est issu de l'aggrégation d'une grande quantité de données historiques réalisée par l'ONG \href{http://berkeleyearth.org/about/}{Berkeley Earth}. Berkeley Earth est associé au Lawrence Berkeley National Laboratory, un laboratoire universitaire de Californie.

    Ce jeu de données a été choisi pour plusieurs raisons :
    \begin{itemize}
        \item il couvre une longue période, allant de 1750 pour les relevés les plus anciens jusqu'à 2015. Dans la mesure où nous nous intéressons à l'évolution des températures sur le long terme, il paraît important de disposer de données remontant suffisamment loin dans le passé.
        \item il provient d'une source fiable : Berkeley Earth est une organisation reconnue par le gouvernement américain dont la direction est majoritairement composée de chercheurs.
        \item les données fournies sont des moyennes mensuelles de température. Cette granularité nous permet de capturer des informations sur l'évolution des températures au fil des saisons, ce que ne permettraient pas des moyennes annuelles par exemple.
    \end{itemize}
    
    Nous nous intéresserons  plus spécifiquement à la \textbf{température mensuelle moyenne sur Terre} de 1860 à nos jours. Cette information est stockée dans la table GlobalTemperatures.csv (cf figure 1).
    


    Les prédictions seront réalisées avec les données de la colonne "LandAndOceanAverageTemperature", qui recense la température moyenne sur Terre chaque mois, en degrés Celsius. Cette valeur est calculée en prenant en compte à la fois les températures mesurées au-dessus des continents et celles mesurées au-dessus des océans. Nous disposons ainsi d'environ 2000 valeurs, dont les premières remontent à 1850. Cette donnée nous donne donc un aperçu de la température sur Terre dans sa globalité pendant un mois donné.
    \begin{figure}[h!]
        \includegraphics[width=12cm]{dataset}
        \centering
        \caption{Caractéristiques statistiques des données de la colonne "LandAndOceanAverageTemperature", issue de la table GlobalTemperatures.csv. Cette colonne contient les températures mensuelles moyennes mesurées au-dessus des continents et des océans}
        \centering
    \end{figure}

    \section{Méthodologie de la veille}
    Cette section présente la façon dont a été réalisée la veille technologique qui nous a permis de sélectionner le modèle d'intelligence artificielle le plus adapté aux besoins de notre client.

    \subsection{Planification}

    La veille a été réalisée sur une période s'étalant sur \textbf{6 semaines}, à raison \textbf{d'une demi-journée} (3 heures) par semaine.

    \subsection{Type de ressources recherché}
    Afin de baser le choix de notre modèle sur des informations fiables, seuls deux types de ressources sont utilisés dans le cadre de cette veille :
    \begin{itemize}
        \item \textbf{les publications universitaires}
        \item les articles et études publiés par des professionnels du domaine de la donnée. Quelques vérifications sont alors réalisées pour \textbf{s'assurer de la fiabilité de la source}. La première d'entre elles consiste à réaliser une recherche sur LinkedIn et Google pour s'assurer que l'auteur est bien un professionnel qui travaille dans le domaine de l'intelligence artificielle. La deuxième vérification réalisée est de faire des recherches sur l'entreprise ou l'organisation à laquelle est affilié l'auteur pour s'assurer qu'il s'agit d'un organisme qui réalise bien des travaux en rapport avec l'intelligence artificielle. Enfin, on vérifiera également que le site où est publié l'article est fiable en se méfiant notamment des sites de publication tels que Medium, où les articles ne sont pas vérifiés avant leur publication.
    \end{itemize}
    Plusieurs moyens ont été employés pour trouver des sources pertinentes qui répondent à ces critères :
    \begin{itemize}
        \item \textbf{\href{https://scholar.google.com/}{Google Scholar}} nous a permis de trouver facilement des publications scientifiques pertinentes par rapport à nos besoins
        \item Des recherches sur Google ont aussi été réalisées pour trouver des sources non-universitaires (publications d'entreprise, blogs de professionnels)
        \item De façon plus informelle, des contacts universitaires travaillant dans le domaine de l'intelligence artificielle ont aussi été sollicités pour demander des conseils ou obtenir l'accès à des publications nécessitant un abonnement.
    \end{itemize}
    \subsection{Présentation de la veille}
    La veille sera présentée comme ceci :
    \begin{enumerate}
        \item Tout d'abord, les modèles considérés dans le cadre de cette veille ainsi que les raisons de leur choix seront présentés
        \item Nous définirons ensuite la métrique de performance utilisée pour évaluer les différents modèles 
        \item Par la suite, les sources utilisées pour réaliser la veille seront présentées. Une liste complète des sources est disponible dans la section \textbf{\hyperref[sec:references]{Références}} à la fin de ce dossier
        \item Nous étudierons ensuite les performances obtenues pour chacun des modèles étudiés dans les sources précédemment présentées. Dans un souci de clarté, ces valeurs seront exposées dans des tableaux qui nous permettront de comparer facilement les différents modèles. Pour chaque source, nous résumerons en quelques phrases les conclusions que l'on peut tirer de ces valeurs. 
        \item Enfin, nous concluerons en choisissant le modèle le plus adapté aux besoins de notre client au vu des résultats de notre veille.
    \end{enumerate}
    \section{Modèles de prédiction envisagés}

    Il existe de nombreux modèles d'IA qui permettent de réaliser des prédictions à partir de séries temporelles. On pourra notamment citer :
    \begin{itemize}
        \item ARIMA
        \item les RNN (Recurrent Neural Networks)
        \item les LSTM (Long-Short-Term Memory)
        \item les CNN (Convolutional Neural Networks)
        \item des modèles basés sur des Random Forests
        \item des KNN
        \item Facebook Prophet
        \item ...
    \end{itemize}
    
    Dans la mesure où il est impossible de tous les traiter, il a fallu faire des choix. Après recherches, les trois modèles qui seront abordés dans ce dossier sont les suivants :
    \begin{itemize}
        \item \textbf{ARIMA}
        \item \textbf{Facebook Prophet}
        \item \textbf{Réseaux LSTM (Long Short-Term Memory)}
    \end{itemize} 
    
    Ces modèles ont été spécifiquement choisis car ils sont très utilisés et qu'il existe donc beaucoup de ressources à leur sujet, y compris en lien avec la prévision de données météorologiques\cite{chen, alsharif, karevan}. Il nous sera ainsi possible de nous baser sur la littérature technique et scientifique existante pour choisir, parmi ces modèles, le plus adapté à nos besoins. Le fonctionnement de chaque modèle est exposé dans la section suivante.

    \subsection{ARIMA (Autoregressive Integrated Moving Average)}

    Plus qu'un modèle, ARIMA est une combinaison de plusieurs modèles :
    \begin{itemize}
        \item \textbf{AR - Autorégression} : ce composant calcule la valeur d'une variable à partir des $p$ dernières observations, $p$ étant un paramètre de notre modèle. Si l'on note $X_t$ la valeur de notre variable à un instant $t$, on a donc :

        \begin{equation}\forall t : X_t = \sum_{i=1}^p \alpha_i X_{t-i} + \epsilon_t \end{equation}
        avec $\epsilon$ l'erreur et $\alpha_1,...\alpha_p$ des réels.
        \item \textbf{MA - Moyenne Mobile} : la moyenne mobile (Moving Average en anglais) exprime une valeur à un instant $t$ comme une combinaison linéaire de $q$ erreurs passées :

        \begin{equation}\forall t : X_t = \epsilon_t + \sum_{i=1}^q \alpha_i \epsilon_{t-i} \end{equation}

        Pour modéliser des séries temporelles de façon plus complexe, on combine le modèle AR et le modèle MA pour créer un modèle dit "ARMA" :

        \begin{equation}\forall t : X_t = AR_t + MA_t = \sum_{i=1}^p \alpha_i X_{t-i} + \epsilon_t + \sum_{i=1}^q \beta_i \epsilon_{t-i} \end{equation}
        avec $\alpha_1,...\alpha_p$ et $\beta_1,...\beta_p$ des réels.
        \item \textbf{I - Intégration} : Le modèle ARMA a une limitation : \textbf{il ne peut modéliser que des séries de donneées stationnaires, c'est-à dire sans tendance à la baisse ou à la hausse}. Pour régler ce problème, on ajoute la composante I au modèle ARMA.
        Cette composante élimine les dépendances des données au temps en différenciant la série temporelle, c'est-à-dire en travaillant avec $X_t - X_{t-1}$ au lieu de $X_t$. Cette différenciation est réalisée $d$ fois, $d$ étant un paramètre de notre modèle ARIMA.

        \textbf{Remarque importante: }Seules certaines séries temporelles peuvent être stationnarisées grâce à la différenciation, il s'agit de celles qui ont une \textbf{racine unitaire}. Il est donc important de réaliser des analyses préliminaires sur nos données pour savoir si la différenciation est adaptée à notre cas de figure avant d'appliquer le modèle ARIMA.

    \end{itemize}
    En plus des paramètres $p$, $d$ et $q$ présentés ci-dessus, le modèle ARIMA peut aussi avoir des paramètres $(P, Q, D)m$ qui décrivent la saisonnalité des données, s'il y en a une. On parle alors de modèle \textbf{SARIMA}. Nous reviendrons plus en détail sur ce point dans le bilan de cette veille.

    \underline{\textbf{Contraintes et prérequis de ce modèle}} : 
    \begin{itemize}
        \item Pour fonctionner, ARIMA doit préalablement être entraîné sur une série de données \textbf{ininterrompue}. Une étape de traitement des données est donc nécessaire s'il manque des valeurs dans notre jeu de données (on pourra par exemple remplacer les valeurs manquantes par la valeur précédente dans la série de données).
        \item Comme nous l'avons vu, ARIMA ne fonctionne pas avec toutes les séries temporelles, uniquement avec celles qui peuvent être stationnarisées par différenciation.
    \end{itemize}
    
    \subsection{Facebook Prophet}

    \href{https://facebook.github.io/prophet/}{Facebook Prophet} est un modèle de prédiction proposé par Meta. 
    Pour réaliser des prédictions, Facebook Prophet considère les données comme la combinaison additive de quatre composants\cite{meta} : 
    \begin{itemize}
        \item la tendance
        \item la saisonnalité annuelle
        \item la saisonnalité hebdomadaire
        \item une liste de jours feriés importants fournie par l'utilisateur.
    \end{itemize}
    \begin{figure}[h!]
        \includegraphics[width=12cm]{facebook_prophet}
        \centering
        \caption{Les différentes composantes de Facebook Prophet. Facebook Prophet fait des prédictions en additionnant quatre séries de données qui représentent la tendance, la saisonnalité annuelle, la saisonnalité hebdomadaire et les jours feriés.}
        \centering
    \end{figure}

    Chacune de ces composantes est gérée par un modèle distinct. La saisonnalité annuelle, par exemple, est modélisée à l'aide de séries de Fourier. Ces quatre modèles sont ensuite combinés additivement.
    
    Facebook Prophet se veut simple à mettre en place et permet de réaliser des prédictions très rapidement. Ce modèle était à l'origine développé pour être utilisé avec les données commerciales auxquelles sont souvent confrontés les data scientists de Meta, ce qui le rend plus adapté à certains types de données spécifiques (données financières ou commerciales par exemple). 

    \underline{\textbf{Contraintes et prérequis de ce modèle}} : 
    \begin{itemize}
        \item Comme ARIMA, Facebook Prophet doit être entraîné avec une série de données ininterrompue
        \item Facebook Prophet est conçu pour être utilisé avec des données qui comportent nécessairement une saisonnalité.
    \end{itemize}
    
    \subsection{Réseaux LSTM (Long Short-Term Memory)}

    Le terme "LSTM" désigne un type de réseau neuronal récurrent (RNN). Les LSTM ont été créés pour pallier aux problèmes de disparition (ou d'explosion, selon les cas) du gradient souvent rencontrés lorsque l'on utilise un réseau récurrent pour traiter de longues séquences de données.
    Par rapport aux RNN classiques, les LSTM ajoutent au réseau un composant supplémentaire qui garde une mémoire de long terme, par opposition à la propagation d'informations naturellement présente dans les RNN via la réutilisation des sorties précédentes en tant que valeurs d'entrée. Les LSTM comportent des composants logiques appelés "gates" qui déterminent quels éléments doivent être stockés dans cette mémoire de long terme. Ces \textit{gates} sont au nombre de trois :
    \begin{itemize}
        \item \textit{l'input gate}, qui contrôle les informations qui sont ajoutées à la cellule mémoire
        \item \textit{la forget gate}, qui définit les informations qui sont supprimées de la cellule mémoire
        \item \textit{l'output gate}, qui contrôle les informations qui sont récupérées depuis la celule mémoire.
    \end{itemize}

    
    \begin{figure}[h!]
        \includegraphics[width=12cm]{RNNvsLSTM}
        \centering
        \caption{Comparaison entre les RNN et les LSTM. En plus de la mémoire déjà présente dans les RNN, les LSTM intègrent une cellule mémoire qui permet de conserver des informations sur le long terme, et donc de traiter de longues séries de données.}
        \centering
    \end{figure}

    Loin d'être propres aux séries temporelles, les LSTM sont utilisés pour répondre à de nombreux problèmes tels que la traduction automatique, la génération de texte ou encore l'analyse de vidéos.

    \underline{\textbf{Contraintes et prérequis de ce modèle}} : 
    \begin{itemize}
        \item Comme pour les modèles précédents, il ne doit pas y avoir de valeurs manquantes dans la série de données
        \item Il est d'usage de stationnariser et normaliser notre série de données avant d'entraîner le modèle.
    \end{itemize}

    \section{Comparaison des modèles}

    \subsection{Métrique utilisée}
    
    Afin de comparer les modèles considérés dans cette étude, il nous faut définir une métrique. Notre choix se portera sur la \textbf{Root-Mean-Square Error} ('racine de l'erreur quadratique moyenne' en français), souvent abrégée RMSE. Elle se définit simplement comme la racine carrée de l'erreur quadratique moyenne/mean-square error (MSE). La MSE est elle-même la moyenne du carré de l'erreur entre une valeur prédite et sa vraie valeur. Si l'on note $\check{\theta}$ la valeur prédite et $\theta$ la vraie valeur, on a :
    
    \begin{equation}RMSE(\check{\theta}) =  \sqrt{MSE(\check{\theta})} = \sqrt{E((\check{\theta} - \theta)^2)}\end{equation}

    Deux raisons expliquent ce choix :
    \begin{itemize}
        \item La RMSE est utilisée comme métrique dans la plupart des sources d'informations qui ont été utilisées dans le cadre de ce dossier
        \item La RMSE est une métrique très courante qui a l'avantage de ne pas donner une importance disproportionnée à quelques valeurs extrêmes, contrairement à la MSE.
    \end{itemize}

    \begin{figure}[h!]
        \includegraphics[width=12cm]{mse}
        \centering
        \caption{Schématisation du concept de la MSE. La MSE correspond ici à la moyenne des surfaces de tous les carrés. Pour obtenir la RMSE il suffit de calculer la racine carrée de la MSE}
        \centering
    \end{figure}

    \subsection{Sources des comparaisons}

    Les comparaisons ci-dessous s'appuient sur les travaux publiés dans des publications scientifiques et des sites web spécialisés dont la liste complète est disponible dans la bibliographie. Parmi ceux-là, on pourra notamment citer :
    \begin{itemize}
        \item Une étude par Ziyar Uzel et al., de la Delft University of Technology\cite{uzel}, qui compare les trois modèles pour la prévision de traffic 
        \item Une comparaison d'ARIMA, de Facebook Prophet et de modèles combinant le LSTM à des CNN, par Lorenzo Menculini et al.\cite{menculini}. Ces modèles sont appliqués à la prédiction de prix de vente de produits alimentaires
        \item Un article du fournisseur de soutions de MLOps Neptune.ai\cite{neptune} qui compare les performances de ces modèles sur des données boursières.
    \end{itemize}

    On pourra remarquer que ces sources ne traitent pas de données météorologiques. Cela s'explique par la difficulté de trouver des sources qui couvrent précisément les trois modèles qui nous intéressent, avec le type de données qui nous intéresse. Ici, notre approche sera donc d'étudier les performances des trois modèles avec plusieurs types de données différents, puis de choisir le modèle le plus performant de manière générale. Le modèle retenu sera ainsi celui qui a le plus de chances d'être performant avec nos données.

    \subsection{Conclusions des sources choisies}
    Bien que toutes les sources n'arrivent pas à un consensus évident, on peut tout de même en dégager plusieurs conclusions.
    L'étude de Ziyar Uzel et al.\cite{uzel} sur les données de traffic montre que :
    \begin{itemize}
        \item ARIMA offre les meilleures performances, suivi des LSTM puis de Facebook Prophet.
        \item Pour expliquer la supériorité d'ARIMA sur les LSTM dans le contexte, l'auteur émet l'hypothèse que le dataset n'est pas assez grand pour les LSTM. Il suppose par ailleurs que le LSTM a de moins bonnes performances car son architecture ne comporte aucune information spatiale qui serait pertinente pour prédire le traffic.
    \end{itemize}
    La table 1 présente la RMSE (\textit{Root Mean-Squared Error}) obtenue avec chaque modèle.
    \begin{table}[h!]
        \begin{center}
            \begin{tabular}{ |c|c| }
                \hline
                 & RMSE \\
                \hline
                ARIMA & 5.5 \\ 
                \hline
                LSTM & 7.13 \\  
                \hline
                Facebook Prophet & 8.02 \\
                \hline
            \end{tabular}
            \caption{RMSE obtenue avec chaque modèle (métrique calculée sur les données de test après finetuning) - Ziyar Uzel et al.}
            \label{table:1}
        \end{center}
    \end{table}
    
    Dans leur étude réalisée à l'Univesité de Perugia sur les prix de vente de produits alimentaires, Lorenzo Menculini et al.\cite{menculini} arrivent aux conclusions suivantes en comparant les performances des trois modèles :
    \begin{itemize}
        \item  Facebook Prophet produit des prédictions moins précises que les LSTM et ARIMA. Cependant, le modèle s'avère très simple et rapide d'utilisation, sans avoir besoin de beaucoup de nettoyage préalable sur les données.
        \item Les LSTM ont les meilleurs performances des trois modèles testés, mais aussi un temps d'entraînement largement plus long.
        \item En termes de temps d'entraînement, le modèle le plus rapide à entraîner est Facebook Prophet, suivi d'ARIMA puis des LSTM.
    \end{itemize}
    Voici les performances calculées pour chaque modèle dans le cadre de cette étude. On remarquera que les modèles ont été testés sur trois produits différents.
    \begin{table}[h!]
        \begin{center}
            \begin{tabular}{ |c| c| c| c| }
                \hline
                 & 1 & 2 & 3 \\
                \hline
                ARIMA & 0.0758 & 0.173 & 0.215 \\ 
                \hline
                LSTM & 0.0613 & 0.162 & 0.200 \\  
                \hline
                Facebook Prophet & 0.0812 & 0.220 & 0.350 \\
                \hline
            \end{tabular}
            \caption{RMSE obtenue avec chaque modèle (métrique calculée sur les données de test après finetuning. Les différentes colonnes correspondent aux jeux de données utilisés, ici des prix de produits alimentaires) - Lorenzo Menculini et al.}
            \label{table:1}
        \end{center}
    \end{table}
    
    Enfin, dans son article publié sur neptune.ai\cite{neptune}, \href{https://www.linkedin.com/in/konstantin-kutzkov-%F0%9F%87%BA%F0%9F%87%A6-10a98667/?originalSubdomain=bg}{Konstantin Kutzkov} compare les trois modèles avec des données boursières. Les conclusions sont les suivantes :
    \begin{itemize}
        \item ARIMA offre les meilleures performances, suivi de Facebook Prophet puis des LSTM
        \item La sélection des hyperparamètres d'ARIMA et des LSTM a un impact considérable sur les performances du modèle.
    \end{itemize}

    La table 3 présente les résultats obtenus par chaque modèle.
    \begin{table}[h!]
        \begin{center}
            \begin{tabular}{ |c|c| }
                \hline
                 & RMSE \\
                \hline
                ARIMA & 224.324 \\ 
                \hline
                LSTM & 694.612 \\  
                \hline
                Facebook Prophet & 317.081 \\
                \hline
            \end{tabular}
            \caption{RMSE obtenue avec chaque modèle (métrique calculée sur les données de test après finetuning) - Konstantin Kutzkov, Neptune.AI}
            \label{table:1}
        \end{center}
    \end{table}

    \subsection{Impact environnemental des différents modèles}
    Bien que notre client souhaite que nous sélectionnions le modèle qui présente les meilleures performances, un autre aspect entre en compte dans le choix du modèle le plus adapté à ses besoins : \textbf{l'impact environnemental des différents modèles}. 

    L'impact environnemental des modèles d'intelligence artificielle a souvent été pointé du doigt ces dernières années, que ce soit dans la presse générale à propos de services comme ChatGPT, ou dans des publications scientifiques\cite{shaji}. Dans un souci d'éco-responsabilité, il est donc important d'évaluer l'impact environnemental de chaque modèle. Cet impact environnemental peut se mesurer en temps de calcul : plus un modèle nécessite un temps d'exécution plus long, plus il consomme d'énergie. On préférera donc si possible choisir un modèle dont les temps d'entraînement et d'inférence sont faibles.

    Bien qu'il soit difficile de trouver des sources qui comparent spécifiquement les temps d'entraînement et d'inférence des modèles que nous étudions, certaines sources nous donnent des indices sur l'impact environnemental de chaque modèle :
    \begin{itemize}
        \item Comme évoqué plus haut, Menculini et al.\cite{menculini} affirment que le modèle le plus rapide à entraîner dans leur étude était Facebook Prophet, suivi d'ARIMA puis des LSTM
        \item Les modèles de deep learning (LSTM compris) sont souvent définis comme exigeants en ce qui concerne les ressources et le temps de calcul nécessaires pour leur entraînement\cite{thompson}.
    \end{itemize}



    \subsection{Bilan et choix du modèle}
    À l'aide des informations ci-dessus, nous pouvons maintenant choisir en toute connaissance de cause le meilleur modèle pour notre problématique.

    Comme nous l'avons dit en début de dossier, les performances sont le critère le plus important dans notre choix, avant la facilité d'utilisation et la rapidité de mise en place. Facebook Prophet paraît donc inadapté à notre problème, au vu de ses performances inférieures à celles des autres modèles.

    En termes de performances, ARIMA et les LSTM paraissent sur un pied d'égalité. Cependant, ARIMA semble être un meilleur choix. En effet, le LSTM a besoin de beaucoup de données pour produire de bons résultats. Or, notre dataset contient une seule série de 2000 échantillons, avec une seule feature. Cela risque de ne pas être suffisant pour obtenir des prédictions fiables. De plus, nous avons vu dans la section précédente qu'ARIMA a un temps d'entraînement moindre comparé à un LSTM. Or, l'une des contraintes du projet est que le client puisse lui-même réentraîner le modèle de prédiction de températures. Le temps d'entraînement a donc une importance particulière si nous souhaitons réduire l'impact environnemental de notre service d'intelligence artificielle. Notre choix se portera donc sur le modèle ARIMA.

    On notera cependant que nos données comportent une saisonnalité dans la mesure où elles représentent des moyennes mensuelles. Nos données suivent donc un cycle de 12 mois. Afin de bien refléter cela, nous déciderons d'implémenter la version saisonnière d'ARIMA que nous avons évoquée lors de la présentation des modèles : le modèle \textbf{SARIMA}, pour \textit{Seasonal ARIMA}.


    
    \section{Mise en place de SARIMA}

    Cette section couvre la mise en place d'un modèle SARIMA, de la sélection des hyperparamètres du modèle à sa mise à disposition sur une API.

    \subsection{Chargement des données et analyses préliminaires}

    La première étape consiste à charger les données que nous utiliserons pour définir les hyperparamètres avec lesquels entraîner notre modèle. Pour cela, nous utiliserons \textbf{pandas} pour lire le CSV qui contient le jeu de données que nous avons présenté au début de ce dossier. Les éventuelles données manquantes dans notre série de données sont remplacées par la moyenne de la valeur précédente et la valeur suivante.

    Comme nous l'avons fait remarquer dans la section qui traite du modèle ARIMA, la différenciation réalisée par le modèle ne permet pas de stationariser tous les types de séries temporelles. Pour nous assurer que la différenciation est bien adaptée à notre cas, nous allons réaliser une différenciation sur nos données, puis tester leur stationarité.
    La stationarité d'une série de données peut être mesurée à l'aide du \textbf{test augmenté de Dickey-Fuller}, ou test ADF. Ce test génère une p-value qui nous servira à déterminer si nos données sont stationnaires ou non. Si notre p-value est inférieure à un seuil fixé à 0.05, nos données sont stationaires. Sinon, elles ne le sont pas.
    Dans notre cas, réaliser une seule différenciation nous a permis de stationnariser nos données (voir la figure 5). Le modèle SARIMA est donc bien applicable à nos données. Par ailleurs, nous avons appris que le paramètre d de notre modèle doit prendre la valeur 1.
    \begin{figure}[h]
        \includegraphics[width=12cm]{differenciating}
        \centering
        \caption{Visualisation de nos données, avant et après différenciation. Ici, réaliser une différenciation a permis de rendre la série temporelle stationnaire}
        \centering
    \end{figure}
    \subsection{Entraînement du modèle et sélection des hyperparamètres}

    Nous utiliserons l'implémentation de modèle SARIMA proposée par le module \href{https://pypi.org/project/pmdarima/}{pmdarima}. Ce module propose la fonction pmdarima.arima.auto\_arima, qui recherche automatiquement les meilleurs paramètres pour notre modèle. Cette fonction cherche à minimiser \textbf{l'AIC}, ou \textit{Akaike Information Critera} de notre modèle, et conserve donc les hyperparamètres qui produisent l'AIC le plus bas. L'AIC est une métrique qui évalue la qualité d'un modèle en prenant en compte son score d'erreur et sa complexité en nombre de paramètres. L'objectif est ici de trouver un modèle qui réalise des prédictions précises tout en limitant les valeurs des hyperparamètres.

    L'exécution de notre code nous donne la sortie suivante : \textbf{"Best model:  ARIMA(1,1,1)(2,0,1)[12] intercept"}. Les hyperparamètres indiqués sont donc ceux que nous utiliserons lors de l'entraînement de notre modèle.

    Nous pouvons maintenant utiliser notre modèle pour réaliser des prédictions. La figure 6 présente les prédictions obtenues par notre modèle en les confrontant avec les valeurs réelles issues de notre jeu de données de test. La lecture de ce graphique nous laisse penser que le modèle SARIMA a bien réussi à prédire les évolutions de notre jeu de données.

    \begin{figure}[h]
        \includegraphics[width=12cm]{forecast}
        \centering
        \caption{Prédictions de notre modèle SARIMA}
        \centering
    \end{figure}

    \subsection{Déploiement}

    Il est temps de déployer le modèle sur une API afin de le mettre à disposition des météorologues. Pour cela, nous utiliserons le module Python \href{https://fastapi.tiangolo.com/}{FastAPI}, qui nous permet de réaliser rapidement des API performantes. Cette API sera ensuite déployée sur \textbf{Azure Web App} grâce aux fonctions d'intégration continue de GitHub. Ainsi, l'API sera automatiquement redéployée à chaque fois que du nouveau code est poussé sur la branche principale du dépôt GitHub du projet.

    Dans notre cas, nous définirons les endpoints suivants :
    \begin{itemize}
        \item \textbf{predict} : permet de prédire toutes les températures moyennes mensuelles jusqu'à une date donnée par l'utilisateur
        \item \textbf{train} : entraîne le modèle SARIMA. Pour cela, l'utilisateur envoie à l'API une liste de températures mensuelles ainsi que la date qui marque le début de la série de températures
        \item \textbf{test} : prédit les températures mensuelles moyennes envoyées par l'utilisateur, puis compare ces prédictions aux températures relevées aux mêmes dates et calcule la RMSE. L'utilisateur doit joindre à cette requête la liste de températures à utiliser pour réaliser le test et la date qui marque le début de cette série.
    \end{itemize}

    Un point crucial est qu'il est nécessaire de \textbf{sécuriser notre API}. En l'état, celle-ci souffre d'un défaut majeur : n'importe qui peut effectuer des requêtes à notre API. Imaginons que quelqu'un sans aucun rapport avec l'organisation de météorologie que nous servons réentraîne le modèle avec ses données, ou sature les capacités de la machine qui héberge notre API en réalisant de nombreuses requêtes simultanément : ce serait un désastre !

    Pour résoudre ce problème, nous allons implémenter un contrôle d'accès à l'aide d'un \textbf{token de sécurité}. Ce token devra être envoyé avec chaque requête à notre API. Seules les requêtes qui présentent un token valide seront traitées. On veillera par ailleurs à ne pas stocker de token en clair dans le code de notre API. Ce token sera enregistré en tant que variable d'environnement sur l'hôte qui exécute le code de notre API sur Azure. Sa valeur est stockée dans un secret GitHub qui est automatiquement transmis à Azure au moment du déploiement.

    Une fois le déploiement effectué, l'API est disponible à l'adresse fournie par Azure (https://vincent-predict-climate-change.azurewebsites.net/ dans notre cas). La figure ci-dessous montre à quoi ressemble l'API une fois déployée sur Azure.

    \begin{figure}[h]
        \includegraphics[width=12cm]{api_azure}
        \centering
        \caption{FastAPI définit automatiquement un endpoint /docs qui nous permet d'envoyer des requêtes à notre API directement depuis le navigateur. Cette figure montre un exemple de requête POST envoyée au endpoint /train}
        \centering
    \end{figure}

    \subsection{Monitorage du modèle}
    
    Une fois notre modèle entraîné, il est important de mettre des mécanismes en place pour s'assurer que ses prédictions restent suffisamment précises sur la durée. En effet, un problème courant lorsque l'on travaille avec des modèles de séries temporelles est le \textbf{data drift}. Le data drift désigne le phénomène qui arrive lorsque la relation entre nos valeurs prédites $Y$ et nos features $X$ change au fil du temps. Le data drift est particulièrement courant lorsque l'on travaille avec des données non stationnaires, comme dans notre cas.
    \begin{figure}[h]
        \includegraphics[width=12cm]{data_drift}
        \centering
        \caption{Illustration du phénomène de "data drift" : il faut surveiller les performances de notre modèle pour le réentraîner lorsque celles-ce se dégradent trop}
        \centering
    \end{figure}
    Une façon de se protéger du data drift est de régulièrement évaluer les prédictions de notre modèle. Si la RMSE de ces prédictions passe au-dessus d'un certain seuil, il devient alors nécessaire de réentraîner notre modèle avec des données récentes. Dans ce projet, nous réalisons ce suivi grâce au script \href{https://github.com/vinpap/predict_climate_change/blob/main/monitoring.py}{monitoring.py}, qui est destiné à être exécuté régulièrement pour vérifier les performances de notre modèle. Lors de son exécution, ce script réalise les tâches suivantes dans l'ordre :
    \begin{itemize}
        \item Le script collecte des données de test mises à disposition par les météorologues de notre client. Ces données peuvent provenir soit d'un fichier CSV, soit d'une base de données PostgreSQL. La source est définie au moment de l'exécution en lisant le contenu du fichier de configuration \href{https://github.com/vinpap/predict_climate_change/blob/main/monitoring_cfg.yml}{monitoring\_cfg.yml}
        \item Ensuite, le script teste le modèle en passant par l'API précédemment présentée et récupère la valeur de la RMSE renvoyée par l'API
        \item Si cette valeur dépasse \textbf{105\% de la RMSE}, le script réentraîne le modèle via le endpoint /train
        \item Nous retestons ensuite le modèle nouvellement réentraîné, puis envoyons automatiquement une alerte par e-mail aux administrateurs du projet pour les informer du résultat.
    \end{itemize}
    \begin{figure}[h]
        \includegraphics[width=12cm]{email_alert}
        \centering
        \caption{Une alerte envoyée par email lorsque le modèle est réentraîné. L'e-mail est envoyé en utilisant un serveur SMTP spécifié dans le fichier de configuration \href{https://github.com/vinpap/predict_climate_change/blob/main/monitoring_cfg.yml}{monitoring\_cfg.yml}}
        \centering
    \end{figure}
    Quelques précisions supplémentaires concernant ce processus :
    \begin{itemize}
        \item Le seuil de 105\% de la RMSE initiale du modèle est stocké dans le fichier de configuration YAML \href{https://github.com/vinpap/predict_climate_change/blob/main/monitoring_cfg.yml}{monitoring\_cfg.yml}. Il est automatiquement calculé et enregistré dans ce fichier lors de la mise en place du monitorage. Ce point sera abordé plus en détail lorsque nous parlerons de la \hyperref[sec:deployment]{documentation qui explique la mise en place de ce projet}.
        \item Certaines informations sensibles telles que les tokens de sécurité de l'API ou du serveur SMTP ne sont pas stockées dans le fichier de configuration du monitorage et doivent être définies en tant que variables d'environnement lors de la mise en place du monitorage.
        \item Comme évoqué plus haut, ce script est destiné à être exécuté régulièrement. Bien qu'il soit possible de faire ça manuellement, il est plus fiable d'automatiser l'exécution du script. Pour les besoins de ce dossier, le script est exécuté grâce au fournisseur de cloud \href{https://www.pythonanywhere.com/}{PythonAnywhere}.
    \end{itemize}
    \begin{figure}[h]
        \includegraphics[width=12cm]{python_anywhere}
        \centering
        \caption{Tableau de bord de PythonAnywhere. Ici, notre script de monitorage est automatiquement exécuté sur le cloud (cf deuxième tâche de la liste). PythonAnywhere ne nous permet d'exécuter une tâche qu'une fois par jour ou par heure. Nous exécutons donc le script de monitorage une fois par jour, mais enregistrons la date du dernier monitorage dans le fichier de configuration et vérifions qu'au moins un mois s'est écoulé depuis le dernier monitorage au moment de l'exécution}
        \centering
    \end{figure}



    



    La figure 8 récapitule le fonctionnement de notre API et ses interactions avec les autres composants, et notamment le script de monitorage.
    \begin{figure}[h]
        \includegraphics[width=12cm]{schema_API_E2}
        \centering
        \caption{Fonctionnement de l'API sur laquelle est déployée notre modèle SARIMA}
        \centering
    \end{figure}

    \section{Documentation}

    [Parler de la doc (à rédiger dans le readme). On la résume quand même brièvement en-dessous]
    \label{sec:deployment}
    \subsection{Déploiement du modèle}
    \subsection{Utilisation du modèle par les météorologues}
    \subsection{Réentraîner le modèle}
    \subsection{Gestion de la sécurité}



    \newpage
    \section*{Conclusion}
    Ce rapport nous a offert une présentation de trois des modèles de prédiction de séries temporelles les plus utilisés : \textbf{ARIMA}, \textbf{Facebook Prophet} et les \textbf{LSTM}. Nous avons ensuite vu pourquoi SARIMA était le modèle le plus adapté à nos besoins. Enfin, nous avons présenté comment déployer notre modèle sur une API REST pour le mettre à disposition des météorologues qui souhaitent réaliser des prédictions concernant les évolutions futures du climat.
    Sur une note plus personnelle, le travail réalisé pour ce projet m'a permis de mieux comprendre le fonctionnement des modèles de séries temporelles ainsi que les caractéristiques propres à ce type de données.

    Pour aller plus loin, n'hésitez pas à consulter le code source développé dans le cadre de ce dossier, disponible sur \href{https://github.com/vinpap/predict_climate_change}{GitHub}.

    \addcontentsline{toc}{section}{Conclusion}

    \newpage
    \begin{thebibliography}{5}
        \bibitem[1]{uzel} ÜZEL, Ziyar. Comparative Analysis of LSTM, ARIMA, and Facebook’s Prophet for Traffic Forecasting: Advancements, Challenges, and Limitations. 2023. 
        \bibitem[2]{menculini} Menculini, L.; Marini, A.; Proietti, M.; Garinei, A.; Bozza, A.; Moretti, C.; Marconi, M. Comparing Prophet and Deep Learning to ARIMA in Forecasting Wholesale Food Prices. Forecasting 2021, 3, 644-662. https://doi.org/10.3390/forecast3030040
        \bibitem[3]{neptune} Kutzkov, Konstantin. ARIMA vs Prophet vs LSTM for Time Series Prediction. 2023. https://neptune.ai/blog/arima-vs-prophet-vs-lstm
        \bibitem[4]{medium} Tum, Phylypo. Overview of Time Series Forecasting from Statistical to Recent ML Approaches. 2020. https://medium.com/@phylypo/overview-of-time-series-forecasting-from-statistical-to-recent-ml-approaches-c51a5dd4656a
        \bibitem[5]{meta} Sean J. Taylor, Ben Letham. Prophet Forecasting at Scale. 2017. https://research.facebook.com/blog/2017/2/prophet-forecasting-at-scale/
        \bibitem[6]{chen} Peng, Chen; Aichen, Niu; Duanyang, Liu; Wei, Jiang; Bin, Ma. Time Series Forecasting of Temperature using SARIMA: An Example from Nanjing. 2018. IOP Conf. Ser.: Mater. Sci. Eng. 394 052024
        \bibitem[7]{alsharif} Alsharif, M.H.; Younes, M.K.; Kim, J. Time Series ARIMA Model for Prediction of Daily and Monthly Average Global Solar Radiation: The Case Study of Seoul, South Korea. Symmetry 2019, 11, 240.
        \bibitem[8]{karevan} Karevan, Zahra; Suykens, Johan A.K. Transductive LSTM for time-series prediction: Am application to weather forecasting. 2020. Neural Networks, Volume 125, Pages 1-9, ISSN 0893-6080.
        \bibitem[9]{shaji} A. Shaji George, A. S. Hovan George, A. S. Gabrio Martin. The Environmental Impact of AI: A Case Study of Water Consumption by Chat GPT. 2023. Partners Universal International Innovation Journal 1 (2):97-104. https://doi.org/10.5281/zenodo.7855594.
        \bibitem[10]{thompson} Thompson, Neil C.; Kristjan Greenewald; Keeheon Lee; Gabriel F. Manso. The Computational Limits of Deep Learning. 2020. arXiv.Org. July 10, 2020. https://arxiv.org/abs/2007.05558.
    \end{thebibliography}
    \addcontentsline{toc}{section}{Références}
    \label{sec:references}

    \end{document}